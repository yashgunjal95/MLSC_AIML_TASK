ğŸ¤– RAG Q&A â€” PDF / Website Retriever (Gemini-Powered)
ğŸ“˜ Overview

RAG Q&A (Retrieval-Augmented Generation) is a Streamlit-based web application that allows users to upload research papers (PDFs) or retrieve website content, and then ask natural language questions about the ingested documents.

It uses Google Gemini 1.5 Flash as the LLM for generating context-aware answers, combined with a vector-based retriever (FAISS) for document search and relevance ranking.

ğŸš€ Features

ğŸ“„ Upload and process PDFs or website data

ğŸ” Ask natural language questions about the uploaded content

âš¡ Powered by Google Gemini 1.5 Flash â€” fast, accurate, and free-tier compatible

ğŸ§© Uses RAG (Retrieval-Augmented Generation) pipeline

ğŸ§  Stores embeddings locally using FAISS

ğŸ–¥ï¸ Clean Streamlit UI

ğŸ§¾ Displays source context for transparency

ğŸ§  How It Works

Document Ingestion
Upload PDFs or retrieve text from a website.
The content is split into smaller, meaningful text chunks using LangChainâ€™s RecursiveCharacterTextSplitter.

Embedding Creation
Each text chunk is converted into numerical embeddings using
GoogleGenerativeAIEmbeddings (models/embedding-001).

Question Answering
When the user enters a query:

The retriever finds the top k relevant text chunks using FAISS.

These chunks are sent to Gemini 1.5 Flash as contextual input.

The model generates an accurate, well-formatted answer.

ğŸ—ï¸ Tech Stack
Component	Technology Used
Frontend	Streamlit
Backend	Python
LLM	Google Gemini 1.5 Flash
Vector Database	FAISS
Embeddings	Google Generative AI Embeddings
Document Processing	LangChain
File Handling	PyPDF2
Environment	.env + .gitignore (secure API key handling)
ğŸ§© Folder Structure
ğŸ“‚ prajwal_rag/
â”œâ”€â”€ app_streamlit.py          # Streamlit frontend
â”œâ”€â”€ rag_engine.py             # Core RAG logic
â”œâ”€â”€ requirements.txt          # Dependencies list
â”œâ”€â”€ README.md                 # Documentation
â”œâ”€â”€ .env.example              # Sample environment file (safe)
â”œâ”€â”€ .gitignore                # Ignored files (e.g., .env, .venv)
â”œâ”€â”€ screenshots/              # Output screenshots
â”‚   â”œâ”€â”€ output1.png
â”‚   â””â”€â”€ output2.png
â””â”€â”€ vectorstore/              # FAISS vector database

âš™ï¸ Installation & Setup
ğŸªœ Step 1: Clone the Repository
git clone https://github.com/your-username/MLSC_AIML_TASK.git
cd MLSC_AIML_TASK/prajwal_rag

ğŸªœ Step 2: Create Virtual Environment
python -m venv .venv
.venv\Scripts\activate        # Windows
source .venv/bin/activate     # macOS/Linux

ğŸªœ Step 3: Install Requirements
pip install -r requirements.txt

ğŸªœ Step 4: Add Gemini API Key

Create a .env file in the same directory:

GEMINI_API_KEY=your_google_api_key_here


(Note: .env is ignored by Git for safety)

ğŸªœ Step 5: Run the Streamlit App
streamlit run app_streamlit.py

ğŸ“¸ Screenshots
ğŸ–¥ï¸ 1. Application Interface

ğŸ§  2. Example Query â€” â€œWho are the authors of the CityZen paper?â€

ğŸ§  Example Query

User Query:

â€œWho are the authors of the CityZen research paper?â€

Generated Answer:

Based on the retrieved document, the authors of the CityZen paper are:
Kalpesh Joshi, Prajwal Bhosale, Shivprasad Bhure, Atharv Bhutada, Bhupen Bibekar, Madhur Biradar, and Aditya Birajdar.

Source: Research Paper2.pdf (Vishwakarma Institute of Technology, Pune)

ğŸ§‘â€ğŸ’» Contributors
Name	Role
Prajwal Bhosale	Developer & Researcher
Google Gemini 1.5 Flash	LLM Backend
LangChain + Streamlit	Framework & Interface
ğŸ Future Enhancements

ğŸ“š Multi-file PDF support

ğŸŒ Website content summarization

ğŸ’¬ Persistent chat memory

â˜ï¸ Deployment on Streamlit Cloud or Hugging Face Spaces

ğŸªª License

This project is open-source and distributed under the MIT License
.

ğŸ’™ Acknowledgment

Built as part of the Microsoft Learn Student Community (MLSC) Internal AI/ML Challenge â€” â€œBuild Your Own Generative AI Appâ€.

Special thanks to MLSC mentors and organizers for guidance and inspiration. ğŸ’«
